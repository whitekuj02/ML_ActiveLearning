{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f16ffa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14bfde3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "role: arn:aws:iam::629515838455:role/service-role/AmazonSageMaker-ExecutionRole-20231201T000262 bucket: gcu-sg02-007-ml2\n",
      "ap-northeast-2\n"
     ]
    }
   ],
   "source": [
    "# setting bucket and session\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"gcu-sg02-007-ml2\"\n",
    "prefix = \"builtin-notebooks/Recomendation-Machine/Explicit\"\n",
    "print(f\"role: {role} bucket: {bucket}\")\n",
    "\n",
    "train_key = 'train.protobuf'\n",
    "train_prefix = '{}/{}'.format(prefix, 'train')\n",
    "s3_train = 's3://{}/{}/train/'.format(bucket,prefix)\n",
    "\n",
    "test_key = 'test.protobuf'\n",
    "test_prefix = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "#ubicación S3 de salida\n",
    "output_prefix = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "my_region = boto3.session.Session().region_name\n",
    "\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "289eef3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Stream_ID</th>\n",
       "      <th>Streamer_username</th>\n",
       "      <th>Time_start</th>\n",
       "      <th>Time_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33842865744</td>\n",
       "      <td>mithrain</td>\n",
       "      <td>154</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33846768288</td>\n",
       "      <td>alptv</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>33886469056</td>\n",
       "      <td>mithrain</td>\n",
       "      <td>587</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>33887624992</td>\n",
       "      <td>wtcn</td>\n",
       "      <td>589</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33890145056</td>\n",
       "      <td>jrokezftw</td>\n",
       "      <td>591</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID    Stream_ID Streamer_username  Time_start  Time_stop\n",
       "0        1  33842865744          mithrain         154        156\n",
       "1        1  33846768288             alptv         166        169\n",
       "2        1  33886469056          mithrain         587        588\n",
       "3        1  33887624992              wtcn         589        591\n",
       "4        1  33890145056         jrokezftw         591        594"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "col_name = [\"User_ID\", \"Stream_ID\", \"Streamer_username\", \"Time_start\", \"Time_stop\"]\n",
    "df = pd.read_csv('./100k_a.csv',header = None, names = col_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34150edd-afac-44e5-83c8-ec37f1431e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 측정 시간 : 6148\n",
      "최소 측정 시간 : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"최대 측정 시간 : \" + str(df[\"Time_stop\"].max()))\n",
    "print(\"최소 측정 시간 : \" + str(df[\"Time_start\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1700b85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Stream_ID</th>\n",
       "      <th>Streamer_username</th>\n",
       "      <th>Time_start</th>\n",
       "      <th>Time_stop</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33842865744</td>\n",
       "      <td>mithrain</td>\n",
       "      <td>154</td>\n",
       "      <td>156</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33846768288</td>\n",
       "      <td>alptv</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>33886469056</td>\n",
       "      <td>mithrain</td>\n",
       "      <td>587</td>\n",
       "      <td>588</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>33887624992</td>\n",
       "      <td>wtcn</td>\n",
       "      <td>589</td>\n",
       "      <td>591</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33890145056</td>\n",
       "      <td>jrokezftw</td>\n",
       "      <td>591</td>\n",
       "      <td>594</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID    Stream_ID Streamer_username  Time_start  Time_stop  rating\n",
       "0        1  33842865744          mithrain         154        156      20\n",
       "1        1  33846768288             alptv         166        169      30\n",
       "2        1  33886469056          mithrain         587        588      10\n",
       "3        1  33887624992              wtcn         589        591      20\n",
       "4        1  33890145056         jrokezftw         591        594      30"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating 추가\n",
    "\n",
    "df[\"rating\"] = (df[\"Time_stop\"] - df[\"Time_start\"]) * 10 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b68864e-e2ea-495e-9d3f-be55a74369a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Streamer_username</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>alptv</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>berkriptepe</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>elraenn</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>eraymaskulen</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>esl_csgo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505153</th>\n",
       "      <td>100000</td>\n",
       "      <td>mckytv</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505154</th>\n",
       "      <td>100000</td>\n",
       "      <td>natehill</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505155</th>\n",
       "      <td>100000</td>\n",
       "      <td>ninja</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505156</th>\n",
       "      <td>100000</td>\n",
       "      <td>replays</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505157</th>\n",
       "      <td>100000</td>\n",
       "      <td>symfuhny</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1505158 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_ID Streamer_username  rating\n",
       "0              1             alptv      30\n",
       "1              1       berkriptepe      30\n",
       "2              1           elraenn      20\n",
       "3              1      eraymaskulen      10\n",
       "4              1          esl_csgo      10\n",
       "...          ...               ...     ...\n",
       "1505153   100000            mckytv      10\n",
       "1505154   100000          natehill      10\n",
       "1505155   100000             ninja      30\n",
       "1505156   100000           replays      10\n",
       "1505157   100000          symfuhny      20\n",
       "\n",
       "[1505158 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping and add rating\n",
    "df = df.groupby(['User_ID', 'Streamer_username']).agg({'rating': 'sum'}).reset_index()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d730ed5-2d3e-407e-91e9-ee2964910703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "6340\n"
     ]
    }
   ],
   "source": [
    "print(df[\"rating\"].min())\n",
    "print(df[\"rating\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c4e2db4-ca0d-444b-a3b0-fd5742cc8e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sampling 100000\n",
    "df = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4627121b-f73a-41ac-8dff-baba49c09e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df.drop(['Stream_ID', 'Time_start', 'Time_stop'], axis = 1)\n",
    "# df.dropna(inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2301e141-34ca-4a53-ac59-ec32575be769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 각 열에 대해 LabelEncoder 생성\n",
    "# user_id_encoder = LabelEncoder()\n",
    "# streamer_username_encoder = LabelEncoder()\n",
    "\n",
    "# # LabelEncoder를 사용하여 데이터 변환\n",
    "# df['User_ID'] = user_id_encoder.fit_transform(df['User_ID'])\n",
    "# df['Streamer_username'] = streamer_username_encoder.fit_transform(df['Streamer_username'])\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea605124-5b55-4cc0-8491-b899aad5beea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train, test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b42e04be-14ac-486c-82dc-915353660262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "enc.fit(df[['User_ID','Streamer_username']])\n",
    "\n",
    "X_train_OH = enc.transform(train_df[['User_ID','Streamer_username']]).astype('float32')\n",
    "Y_train_OH = train_df['rating'].astype('float32')  \n",
    "\n",
    "X_test_OH = enc.transform(test_df[['User_ID','Streamer_username']]).astype('float32')\n",
    "Y_test_OH = test_df['rating'].astype('float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf005beb-489b-4454-8a40-b29e677f153b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_IDs: [     2      7     14 ...  99997  99999 100000]\n",
      "Streamer_usernames: ['030_hi' '080808080' '0_doublezero_0' ... 'zzzerrr' 'zzzireael'\n",
      " 'zzztantrikazzz']\n"
     ]
    }
   ],
   "source": [
    "# User_ID Streamer_username unique data\n",
    "user_ids = enc.categories_[0]\n",
    "streamer_usernames = enc.categories_[1] \n",
    "\n",
    "print(\"User_IDs:\", user_ids)\n",
    "print(\"Streamer_usernames:\", streamer_usernames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6549145-69bf-4f96-ac66-76aaba4f29dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20060)\t1.0\n",
      "  (0, 65418)\t1.0\n",
      "  (1, 17878)\t1.0\n",
      "  (1, 82138)\t1.0\n",
      "  (2, 13104)\t1.0\n",
      "  (2, 71128)\t1.0\n",
      "  (3, 36499)\t1.0\n",
      "  (3, 77585)\t1.0\n",
      "  (4, 22688)\t1.0\n",
      "  (4, 82166)\t1.0\n",
      "  (5, 32560)\t1.0\n",
      "  (5, 64781)\t1.0\n",
      "  (6, 47248)\t1.0\n",
      "  (6, 67002)\t1.0\n",
      "  (7, 25214)\t1.0\n",
      "  (7, 76043)\t1.0\n",
      "  (8, 20818)\t1.0\n",
      "  (8, 70442)\t1.0\n",
      "  (9, 33288)\t1.0\n",
      "  (9, 71539)\t1.0\n",
      "  (10, 14118)\t1.0\n",
      "  (10, 76526)\t1.0\n",
      "  (11, 46378)\t1.0\n",
      "  (11, 61261)\t1.0\n",
      "  (12, 44053)\t1.0\n",
      "  :\t:\n",
      "  (79987, 67969)\t1.0\n",
      "  (79988, 4298)\t1.0\n",
      "  (79988, 70145)\t1.0\n",
      "  (79989, 24959)\t1.0\n",
      "  (79989, 65660)\t1.0\n",
      "  (79990, 17155)\t1.0\n",
      "  (79990, 66507)\t1.0\n",
      "  (79991, 29248)\t1.0\n",
      "  (79991, 56865)\t1.0\n",
      "  (79992, 23542)\t1.0\n",
      "  (79992, 62179)\t1.0\n",
      "  (79993, 34313)\t1.0\n",
      "  (79993, 62212)\t1.0\n",
      "  (79994, 20976)\t1.0\n",
      "  (79994, 64435)\t1.0\n",
      "  (79995, 24490)\t1.0\n",
      "  (79995, 67300)\t1.0\n",
      "  (79996, 33960)\t1.0\n",
      "  (79996, 68279)\t1.0\n",
      "  (79997, 9440)\t1.0\n",
      "  (79997, 78306)\t1.0\n",
      "  (79998, 41314)\t1.0\n",
      "  (79998, 53175)\t1.0\n",
      "  (79999, 46273)\t1.0\n",
      "  (79999, 78347)\t1.0\n",
      "  (0, 6301)\t1.0\n",
      "  (0, 55976)\t1.0\n",
      "  (1, 48239)\t1.0\n",
      "  (1, 76103)\t1.0\n",
      "  (2, 17977)\t1.0\n",
      "  (2, 61801)\t1.0\n",
      "  (3, 47081)\t1.0\n",
      "  (3, 69575)\t1.0\n",
      "  (4, 637)\t1.0\n",
      "  (4, 60487)\t1.0\n",
      "  (5, 10968)\t1.0\n",
      "  (5, 56798)\t1.0\n",
      "  (6, 27285)\t1.0\n",
      "  (6, 54709)\t1.0\n",
      "  (7, 26957)\t1.0\n",
      "  (7, 62011)\t1.0\n",
      "  (8, 40823)\t1.0\n",
      "  (8, 79485)\t1.0\n",
      "  (9, 14776)\t1.0\n",
      "  (9, 77919)\t1.0\n",
      "  (10, 6002)\t1.0\n",
      "  (10, 56798)\t1.0\n",
      "  (11, 42463)\t1.0\n",
      "  (11, 56665)\t1.0\n",
      "  (12, 23575)\t1.0\n",
      "  :\t:\n",
      "  (19987, 72212)\t1.0\n",
      "  (19988, 39300)\t1.0\n",
      "  (19988, 61781)\t1.0\n",
      "  (19989, 31032)\t1.0\n",
      "  (19989, 62286)\t1.0\n",
      "  (19990, 8404)\t1.0\n",
      "  (19990, 56741)\t1.0\n",
      "  (19991, 16425)\t1.0\n",
      "  (19991, 59232)\t1.0\n",
      "  (19992, 31069)\t1.0\n",
      "  (19992, 76064)\t1.0\n",
      "  (19993, 27187)\t1.0\n",
      "  (19993, 53525)\t1.0\n",
      "  (19994, 42724)\t1.0\n",
      "  (19994, 82074)\t1.0\n",
      "  (19995, 31804)\t1.0\n",
      "  (19995, 74701)\t1.0\n",
      "  (19996, 16294)\t1.0\n",
      "  (19996, 74814)\t1.0\n",
      "  (19997, 18569)\t1.0\n",
      "  (19997, 64441)\t1.0\n",
      "  (19998, 4163)\t1.0\n",
      "  (19998, 56503)\t1.0\n",
      "  (19999, 3789)\t1.0\n",
      "  (19999, 80971)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_OH)\n",
    "print(X_test_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f529b889-f571-4923-a2d6-aed7dcba36c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        700.0\n",
      "1         10.0\n",
      "2        120.0\n",
      "3         70.0\n",
      "4         10.0\n",
      "         ...  \n",
      "79995     30.0\n",
      "79996    110.0\n",
      "79997     20.0\n",
      "79998    370.0\n",
      "79999     10.0\n",
      "Name: rating, Length: 80000, dtype: float32\n",
      "0        30.0\n",
      "1        10.0\n",
      "2        20.0\n",
      "3        50.0\n",
      "4        80.0\n",
      "         ... \n",
      "19995    10.0\n",
      "19996    30.0\n",
      "19997    10.0\n",
      "19998    50.0\n",
      "19999    20.0\n",
      "Name: rating, Length: 20000, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_OH.sort_index())\n",
    "print(Y_test_OH.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6550989c-07d9-464a-8996-cbd57188f886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:82636 \n",
      "Rows:80000 \n"
     ]
    }
   ],
   "source": [
    "# row and col number\n",
    "columns = X_train_OH.shape[1]\n",
    "\n",
    "print(\"Columns:{} \".format(X_train_OH.shape[1]))\n",
    "print(\"Rows:{} \".format(X_train_OH.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3a13d34-36dc-4af3-bf06-4a2f75cac3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.to_csv('train_data.csv', mode=\"w\", header=False, index=False)\n",
    "# test_df.to_csv('test_data.csv', mode=\"w\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f50de712-94bd-4760-8d63-76976620ab10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://gcu-sg02-007-ml2/builtin-notebooks/Recomendation-Machine/Explicit/train/train.protobuf\n",
      "s3://gcu-sg02-007-ml2/builtin-notebooks/Recomendation-Machine/Explicit/test/test.protobuf\n",
      "Output: s3://gcu-sg02-007-ml2/builtin-notebooks/Recomendation-Machine/Explicit/output\n"
     ]
    }
   ],
   "source": [
    "# save data at bucket\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    # Y = Y.reset_index(drop=True)\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket, obj)\n",
    "  \n",
    "train_data = writeDatasetToProtobuf(X_train_OH, Y_train_OH, bucket, train_prefix, train_key)    \n",
    "test_data = writeDatasetToProtobuf(X_test_OH, Y_test_OH, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7b3087e-73b8-427f-b8a5-4e05a627a252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bucket에 올리기\n",
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train_data.csv')).upload_file('train_data.csv')\n",
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test_data.csv')).upload_file('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b311148-0f1a-4049-a71b-5d3d3348f38e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_data, content_type=\"text/csv\")\n",
    "# s3_input_test = sagemaker.inputs.TrainingInput(s3_data=test_data, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bad6047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# sagemaker model\n",
    "instance_type='ml.m5.large'\n",
    "fm = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"factorization-machines\"),\n",
    "                                   role, \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type=instance_type,\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=columns,\n",
    "                      predictor_type='regressor',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8052467b-2063-4a03-b12e-453274a8277b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2023-12-02-10-18-06-961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-02 10:18:07 Starting - Starting the training job...\n",
      "2023-12-02 10:18:23 Starting - Preparing the instances for training......\n",
      "2023-12-02 10:19:29 Downloading - Downloading input data......\n",
      "2023-12-02 10:20:09 Training - Downloading the training image........\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '100', 'feature_dim': '82636', 'mini_batch_size': '1000', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Final configuration: {'epochs': '100', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '82636', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 WARNING 139702296962880] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Using default worker.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:51.791] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:51.799] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 66760}\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] nvidia-smi: took 0.037 seconds to run.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:51 INFO 139702296962880] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512511.7801163, \"EndTime\": 1701512511.8396628, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 43.329715728759766, \"count\": 1, \"min\": 43.329715728759766, \"max\": 43.329715728759766}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512511.8397975, \"EndTime\": 1701512511.8398445, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[10:21:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.363.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[10:21:51] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.363.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m[10:21:52] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.363.0/AL2_x86_64/generic-flavor/src/src/operator/././../common/utils.h:450: Optimizer with lazy_update = True detected. Be aware that lazy update with row_sparse gradient is different from standard update, and may lead to different empirical results. See https://mxnet.incubator.apache.org/api/python/optimization/optimization.html for more details.\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=138.4787853788442\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=19176.374\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=63.61032421875\u001b[0m\n",
      "\n",
      "2023-12-02 10:21:44 Training - Training image download completed. Training in progress.\u001b[34m[2023-12-02 10:21:53.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1167, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=144.83726445566418\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, train mse <loss>=20977.833175\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=60.12069921875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512511.839737, \"EndTime\": 1701512513.4106517, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"update.time\": {\"sum\": 1570.5070495605469, \"count\": 1, \"min\": 1570.5070495605469, \"max\": 1570.5070495605469}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512511.840111, \"EndTime\": 1701512513.4110928, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 81000.0, \"count\": 1, \"min\": 81000, \"max\": 81000}, \"Total Batches Seen\": {\"sum\": 81.0, \"count\": 1, \"min\": 81, \"max\": 81}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=50919.99287366013 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=135.05887605040996\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=18240.9\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=55.7762734375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:54.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 1254, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=141.85280614954362\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, train mse <loss>=20122.2186125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=53.975394921875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512513.4107208, \"EndTime\": 1701512514.702244, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1290.7500267028809, \"count\": 1, \"min\": 1290.7500267028809, \"max\": 1290.7500267028809}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512513.4114308, \"EndTime\": 1701512514.7024884, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 161000.0, \"count\": 1, \"min\": 161000, \"max\": 161000}, \"Total Batches Seen\": {\"sum\": 161.0, \"count\": 1, \"min\": 161, \"max\": 161}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=61955.01238474982 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=132.20127079570756\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=17477.176\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=52.1963359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:56.015] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 1291, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=139.39084147640403\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, train mse <loss>=19429.8066875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=52.278638232421876\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512514.7023199, \"EndTime\": 1701512516.0168064, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1313.9586448669434, \"count\": 1, \"min\": 1313.9586448669434, \"max\": 1313.9586448669434}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512514.7028182, \"EndTime\": 1701512516.0175211, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 241000.0, \"count\": 1, \"min\": 241000, \"max\": 241000}, \"Total Batches Seen\": {\"sum\": 241.0, \"count\": 1, \"min\": 241, \"max\": 241}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=60831.503039451134 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=129.87005043504064\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=16866.23\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=50.99719921875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:57.091] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1055, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=137.404786534167\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, train mse <loss>=18880.0753625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=52.29446567382813\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512516.0171278, \"EndTime\": 1701512517.0925326, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1074.2015838623047, \"count\": 1, \"min\": 1074.2015838623047, \"max\": 1074.2015838623047}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512516.0183024, \"EndTime\": 1701512517.0927796, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 321000.0, \"count\": 1, \"min\": 321000, \"max\": 321000}, \"Total Batches Seen\": {\"sum\": 321.0, \"count\": 1, \"min\": 321, \"max\": 321}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=74445.21677780207 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=128.01114404613372\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=16386.853\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=51.4261328125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:58.022] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 912, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=135.83877332153733\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, train mse <loss>=18452.1723375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=52.99428012695312\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512517.0926085, \"EndTime\": 1701512518.0227234, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 929.6226501464844, \"count\": 1, \"min\": 929.6226501464844, \"max\": 929.6226501464844}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512517.0930707, \"EndTime\": 1701512518.02297, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 401000.0, \"count\": 1, \"min\": 401000, \"max\": 401000}, \"Total Batches Seen\": {\"sum\": 401.0, \"count\": 1, \"min\": 401, \"max\": 401}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86017.64013127302 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=126.56443023219438\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=16018.555\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=52.53543359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:58.964] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 924, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=134.63348849747598\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, train mse <loss>=18126.176225\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=54.29148051757812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512518.0227942, \"EndTime\": 1701512518.965015, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 941.7233467102051, \"count\": 1, \"min\": 941.7233467102051, \"max\": 941.7233467102051}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512518.023263, \"EndTime\": 1701512518.965356, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 481000.0, \"count\": 1, \"min\": 481000, \"max\": 481000}, \"Total Batches Seen\": {\"sum\": 481.0, \"count\": 1, \"min\": 481, \"max\": 481}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=84902.87685319385 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=125.46720687095892\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=15742.02\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=53.83811328125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:21:59.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 901, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=133.72922371344268\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, train mse <loss>=17883.505275\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=55.64272353515625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512518.9651327, \"EndTime\": 1701512519.8848867, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 919.121265411377, \"count\": 1, \"min\": 919.121265411377, \"max\": 919.121265411377}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512518.965738, \"EndTime\": 1701512519.885206, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 561000.0, \"count\": 1, \"min\": 561000, \"max\": 561000}, \"Total Batches Seen\": {\"sum\": 561.0, \"count\": 1, \"min\": 561, \"max\": 561}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86987.39821485513 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=124.65737443087752\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=15539.461\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:21:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=55.37519140625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:00.808] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 904, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=133.06862458896913\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, train mse <loss>=17707.25885\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=57.17784482421875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512519.8849926, \"EndTime\": 1701512520.8093934, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 923.7468242645264, \"count\": 1, \"min\": 923.7468242645264, \"max\": 923.7468242645264}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512519.8856194, \"EndTime\": 1701512520.8097084, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 641000.0, \"count\": 1, \"min\": 641000, \"max\": 641000}, \"Total Batches Seen\": {\"sum\": 641.0, \"count\": 1, \"min\": 641, \"max\": 641}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86553.150415181 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=124.07633134486206\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=15394.936\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=56.8343203125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:02.097] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 1271, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=132.59902969101998\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, train mse <loss>=17582.502675\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=58.52487143554688\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512520.8094838, \"EndTime\": 1701512522.0984182, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1288.3148193359375, \"count\": 1, \"min\": 1288.3148193359375, \"max\": 1288.3148193359375}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512520.8100665, \"EndTime\": 1701512522.0988858, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 721000.0, \"count\": 1, \"min\": 721000, \"max\": 721000}, \"Total Batches Seen\": {\"sum\": 721.0, \"count\": 1, \"min\": 721, \"max\": 721}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=62057.0709084547 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=123.67132246402154\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=15294.596\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=58.12625390625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:03.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1453, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=132.27419003721022\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, train mse <loss>=17496.46135\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=59.83824418945313\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512522.098509, \"EndTime\": 1701512523.5738304, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1474.2677211761475, \"count\": 1, \"min\": 1474.2677211761475, \"max\": 1474.2677211761475}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512522.099515, \"EndTime\": 1701512523.5742242, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 801000.0, \"count\": 1, \"min\": 801000, \"max\": 801000}, \"Total Batches Seen\": {\"sum\": 801.0, \"count\": 1, \"min\": 801, \"max\": 801}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=54241.29841204719 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=123.39700158431728\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=15226.82\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=59.388203125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:04.835] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 1237, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=132.05526707594817\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, train mse <loss>=17438.5935625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=61.02403779296875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512523.5739439, \"EndTime\": 1701512524.836579, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1261.8720531463623, \"count\": 1, \"min\": 1261.8720531463623, \"max\": 1261.8720531463623}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512523.5746825, \"EndTime\": 1701512524.8369315, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 881000.0, \"count\": 1, \"min\": 881000, \"max\": 881000}, \"Total Batches Seen\": {\"sum\": 881.0, \"count\": 1, \"min\": 881, \"max\": 881}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=63371.61190798335 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=123.21614748075838\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=15182.219\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=60.43236328125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:05.767] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 911, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=131.91108065663022\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, train mse <loss>=17400.5332\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=61.996440087890626\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512524.8366396, \"EndTime\": 1701512525.7684493, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 931.0896396636963, \"count\": 1, \"min\": 931.0896396636963, \"max\": 931.0896396636963}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512524.8373334, \"EndTime\": 1701512525.7687418, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 961000.0, \"count\": 1, \"min\": 961000, \"max\": 961000}, \"Total Batches Seen\": {\"sum\": 961.0, \"count\": 1, \"min\": 961, \"max\": 961}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=85878.83393227294 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=123.09961819599604\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=15153.516\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=61.279765625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:06.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 904, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=131.81771893414026\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, train mse <loss>=17375.911025\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=62.77828818359375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512525.768546, \"EndTime\": 1701512526.692936, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 923.8088130950928, \"count\": 1, \"min\": 923.8088130950928, \"max\": 923.8088130950928}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512525.7690969, \"EndTime\": 1701512526.693241, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1041000.0, \"count\": 1, \"min\": 1041000, \"max\": 1041000}, \"Total Batches Seen\": {\"sum\": 1041.0, \"count\": 1, \"min\": 1041, \"max\": 1041}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86547.4352714561 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=123.02564366830194\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=15135.309\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=61.953640625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:07.605] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 896, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=131.75766282269885\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, train mse <loss>=17360.0817125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=63.398067822265624\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512526.6930296, \"EndTime\": 1701512527.6068015, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 913.140058517456, \"count\": 1, \"min\": 913.140058517456, \"max\": 913.140058517456}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512526.6936173, \"EndTime\": 1701512527.6071284, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1121000.0, \"count\": 1, \"min\": 1121000, \"max\": 1121000}, \"Total Batches Seen\": {\"sum\": 1121.0, \"count\": 1, \"min\": 1121, \"max\": 1121}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87558.79951474564 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=122.97870140800805\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=15123.761\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=62.50234375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:08.491] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 868, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=131.71864783507309\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, train mse <loss>=17349.8021875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=63.921969873046876\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512527.6068828, \"EndTime\": 1701512528.4924939, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 884.9687576293945, \"count\": 1, \"min\": 884.9687576293945, \"max\": 884.9687576293945}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512527.6074998, \"EndTime\": 1701512528.492817, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1201000.0, \"count\": 1, \"min\": 1201000, \"max\": 1201000}, \"Total Batches Seen\": {\"sum\": 1201.0, \"count\": 1, \"min\": 1201, \"max\": 1201}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90346.55916621814 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=122.94825334261564\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=15116.273\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=62.97276953125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:09.395] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 885, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=131.69246490403313\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, train mse <loss>=17342.9053125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=64.33454755859375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512528.4925525, \"EndTime\": 1701512529.396626, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 903.3162593841553, \"count\": 1, \"min\": 903.3162593841553, \"max\": 903.3162593841553}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512528.493287, \"EndTime\": 1701512529.3969998, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1281000.0, \"count\": 1, \"min\": 1281000, \"max\": 1281000}, \"Total Batches Seen\": {\"sum\": 1281.0, \"count\": 1, \"min\": 1281, \"max\": 1281}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88513.21917383626 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=122.92747048564857\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=15111.163\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=63.32294140625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:10.341] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 928, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=131.67385541556834\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, train mse <loss>=17338.0042\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=64.63840551757812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512529.3966854, \"EndTime\": 1701512530.3428104, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 945.3737735748291, \"count\": 1, \"min\": 945.3737735748291, \"max\": 945.3737735748291}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512529.3973742, \"EndTime\": 1701512530.3431556, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1361000.0, \"count\": 1, \"min\": 1361000, \"max\": 1361000}, \"Total Batches Seen\": {\"sum\": 1361.0, \"count\": 1, \"min\": 1361, \"max\": 1361}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=84571.70279455546 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=122.91210680807647\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=15107.386\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=63.5771640625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:11.227] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 866, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=131.65958230413767\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, train mse <loss>=17334.2456125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=64.85655395507813\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512530.3428695, \"EndTime\": 1701512531.2290394, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 885.3898048400879, \"count\": 1, \"min\": 885.3898048400879, \"max\": 885.3898048400879}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512530.3436227, \"EndTime\": 1701512531.2293134, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1441000.0, \"count\": 1, \"min\": 1441000, \"max\": 1441000}, \"Total Batches Seen\": {\"sum\": 1441.0, \"count\": 1, \"min\": 1441, \"max\": 1441}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90310.32774147151 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=122.8996704633499\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=15104.329\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=63.756734375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:12.116] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 881, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=131.64773213010545\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, train mse <loss>=17331.125375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=65.00882221679687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512531.2291145, \"EndTime\": 1701512532.116743, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 887.1097564697266, \"count\": 1, \"min\": 887.1097564697266, \"max\": 887.1097564697266}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512531.229606, \"EndTime\": 1701512532.1170313, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1521000.0, \"count\": 1, \"min\": 1521000, \"max\": 1521000}, \"Total Batches Seen\": {\"sum\": 1521.0, \"count\": 1, \"min\": 1521, \"max\": 1521}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90133.74564634955 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=122.88874643351197\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=15101.644\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=63.87968359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:13.007] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 884, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=131.6372066039841\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, train mse <loss>=17328.3541625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=65.111703125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512532.116837, \"EndTime\": 1701512533.0083897, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 891.0491466522217, \"count\": 1, \"min\": 891.0491466522217, \"max\": 891.0491466522217}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512532.117316, \"EndTime\": 1701512533.0087476, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1601000.0, \"count\": 1, \"min\": 1601000, \"max\": 1601000}, \"Total Batches Seen\": {\"sum\": 1601.0, \"count\": 1, \"min\": 1601, \"max\": 1601}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89727.13717587512 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=122.87855386518837\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=15099.139\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=63.96073046875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:13.937] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 886, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=131.62739214350484\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, train mse <loss>=17325.7703625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=65.17847280273438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512533.0084503, \"EndTime\": 1701512533.9382746, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 929.0847778320312, \"count\": 1, \"min\": 929.0847778320312, \"max\": 929.0847778320312}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512533.009161, \"EndTime\": 1701512533.938648, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1681000.0, \"count\": 1, \"min\": 1681000, \"max\": 1681000}, \"Total Batches Seen\": {\"sum\": 1681.0, \"count\": 1, \"min\": 1681, \"max\": 1681}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86052.7373337741 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=122.86870228011689\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=15096.718\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=64.01154296875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:14.830] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 875, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=131.61795156816567\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, train mse <loss>=17323.285175\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=65.21951909179687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512533.9383812, \"EndTime\": 1701512534.8315156, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 892.4295902252197, \"count\": 1, \"min\": 892.4295902252197, \"max\": 892.4295902252197}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512533.9390588, \"EndTime\": 1701512534.8318372, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1761000.0, \"count\": 1, \"min\": 1761000, \"max\": 1761000}, \"Total Batches Seen\": {\"sum\": 1761.0, \"count\": 1, \"min\": 1761, \"max\": 1761}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89586.08419285457 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=122.85898420547029\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=15094.33\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=64.04110546875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:15.725] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 869, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=131.60869951298812\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, train mse <loss>=17320.8497875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=65.24268803710937\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512534.831625, \"EndTime\": 1701512535.7269697, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 894.7217464447021, \"count\": 1, \"min\": 894.7217464447021, \"max\": 894.7217464447021}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512534.8322222, \"EndTime\": 1701512535.72734, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1841000.0, \"count\": 1, \"min\": 1841000, \"max\": 1841000}, \"Total Batches Seen\": {\"sum\": 1841.0, \"count\": 1, \"min\": 1841, \"max\": 1841}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89358.34006650258 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=122.84929792229177\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=15091.95\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=64.05605859375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:16.623] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 878, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=131.59953248397198\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, train mse <loss>=17318.43695\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=65.25372431640625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512535.727043, \"EndTime\": 1701512536.6244235, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 896.620512008667, \"count\": 1, \"min\": 896.620512008667, \"max\": 896.620512008667}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512535.727726, \"EndTime\": 1701512536.624751, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1921000.0, \"count\": 1, \"min\": 1921000, \"max\": 1921000}, \"Total Batches Seen\": {\"sum\": 1921.0, \"count\": 1, \"min\": 1921, \"max\": 1921}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89169.19899431492 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=122.83959866427438\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=15089.567\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=64.0612421875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:17.522] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 881, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=131.5903958311548\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, train mse <loss>=17316.032275\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=65.25675131835938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512536.624506, \"EndTime\": 1701512537.5228746, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 897.7265357971191, \"count\": 1, \"min\": 897.7265357971191, \"max\": 897.7265357971191}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512536.6251185, \"EndTime\": 1701512537.5231156, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2001000.0, \"count\": 1, \"min\": 2001000, \"max\": 2001000}, \"Total Batches Seen\": {\"sum\": 2001.0, \"count\": 1, \"min\": 2001, \"max\": 2001}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89073.92344954862 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, batch=0 train rmse <loss>=122.8298620043188\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, batch=0 train mse <loss>=15087.175\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, batch=0 train absolute_loss <loss>=64.06004296875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:18.410] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 871, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, train rmse <loss>=131.5812547439794\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, train mse <loss>=17313.6266\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=25, train absolute_loss <loss>=65.25460087890625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512537.522947, \"EndTime\": 1701512538.4114993, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.0772590637207, \"count\": 1, \"min\": 888.0772590637207, \"max\": 888.0772590637207}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512537.523393, \"EndTime\": 1701512538.411749, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2081000.0, \"count\": 1, \"min\": 2081000, \"max\": 2081000}, \"Total Batches Seen\": {\"sum\": 2081.0, \"count\": 1, \"min\": 2081, \"max\": 2081}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 27.0, \"count\": 1, \"min\": 27, \"max\": 27}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90039.92629239871 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, batch=0 train rmse <loss>=122.82007572054334\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, batch=0 train mse <loss>=15084.771\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, batch=0 train absolute_loss <loss>=64.0547421875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:19.309] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 880, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, train rmse <loss>=131.5720908380649\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, train mse <loss>=17311.2150875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=26, train absolute_loss <loss>=65.2491798828125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512538.411575, \"EndTime\": 1701512539.3104916, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 898.3900547027588, \"count\": 1, \"min\": 898.3900547027588, \"max\": 898.3900547027588}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512538.412029, \"EndTime\": 1701512539.3108823, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2161000.0, \"count\": 1, \"min\": 2161000, \"max\": 2161000}, \"Total Batches Seen\": {\"sum\": 2161.0, \"count\": 1, \"min\": 2161, \"max\": 2161}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88988.71517533236 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, batch=0 train rmse <loss>=122.81023980108499\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, batch=0 train mse <loss>=15082.355\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, batch=0 train absolute_loss <loss>=64.04687890625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:20.254] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 927, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, train rmse <loss>=131.56289508254218\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, train mse <loss>=17308.7953625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=27, train absolute_loss <loss>=65.24173017578126\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512539.3105588, \"EndTime\": 1701512540.2555032, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 944.1511631011963, \"count\": 1, \"min\": 944.1511631011963, \"max\": 944.1511631011963}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512539.3113136, \"EndTime\": 1701512540.2558508, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2241000.0, \"count\": 1, \"min\": 2241000, \"max\": 2241000}, \"Total Batches Seen\": {\"sum\": 2241.0, \"count\": 1, \"min\": 2241, \"max\": 2241}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=84673.99267029527 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, batch=0 train rmse <loss>=122.80034201906768\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, batch=0 train mse <loss>=15079.924\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, batch=0 train absolute_loss <loss>=64.037421875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:21.141] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 58, \"duration\": 868, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, train rmse <loss>=131.55366124703636\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, train mse <loss>=17306.3657875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=28, train absolute_loss <loss>=65.23305063476562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512540.2555869, \"EndTime\": 1701512541.1424725, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 886.0819339752197, \"count\": 1, \"min\": 886.0819339752197, \"max\": 886.0819339752197}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512540.2563627, \"EndTime\": 1701512541.1428874, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2321000.0, \"count\": 1, \"min\": 2321000, \"max\": 2321000}, \"Total Batches Seen\": {\"sum\": 2321.0, \"count\": 1, \"min\": 2321, \"max\": 2321}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90227.95481722715 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, batch=0 train rmse <loss>=122.79039457547158\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, batch=0 train mse <loss>=15077.481\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, batch=0 train absolute_loss <loss>=64.026984375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:22.037] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 60, \"duration\": 876, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, train rmse <loss>=131.54438633024216\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, train mse <loss>=17303.925575\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=29, train absolute_loss <loss>=65.22361357421875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512541.142558, \"EndTime\": 1701512542.0385482, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 895.3506946563721, \"count\": 1, \"min\": 895.3506946563721, \"max\": 895.3506946563721}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512541.1431563, \"EndTime\": 1701512542.038993, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2401000.0, \"count\": 1, \"min\": 2401000, \"max\": 2401000}, \"Total Batches Seen\": {\"sum\": 2401.0, \"count\": 1, \"min\": 2401, \"max\": 2401}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89286.19857607989 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, batch=0 train rmse <loss>=122.78039338591483\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, batch=0 train mse <loss>=15075.025\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, batch=0 train absolute_loss <loss>=64.01593359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:22.926] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 871, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, train rmse <loss>=131.53506980079496\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, train mse <loss>=17301.4745875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=30, train absolute_loss <loss>=65.2137208984375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512542.0386796, \"EndTime\": 1701512542.927645, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.16237449646, \"count\": 1, \"min\": 888.16237449646, \"max\": 888.16237449646}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512542.0394046, \"EndTime\": 1701512542.9280944, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2481000.0, \"count\": 1, \"min\": 2481000, \"max\": 2481000}, \"Total Batches Seen\": {\"sum\": 2481.0, \"count\": 1, \"min\": 2481, \"max\": 2481}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90007.12719371777 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, batch=0 train rmse <loss>=122.7703425099075\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, batch=0 train mse <loss>=15072.557\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, batch=0 train absolute_loss <loss>=64.0044921875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:23.821] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 64, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, train rmse <loss>=131.52571288535182\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, train mse <loss>=17299.01315\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=31, train absolute_loss <loss>=65.20353525390625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512542.9277472, \"EndTime\": 1701512543.8225935, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 894.0253257751465, \"count\": 1, \"min\": 894.0253257751465, \"max\": 894.0253257751465}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512542.9285188, \"EndTime\": 1701512543.8228447, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2561000.0, \"count\": 1, \"min\": 2561000, \"max\": 2561000}, \"Total Batches Seen\": {\"sum\": 2561.0, \"count\": 1, \"min\": 2561, \"max\": 2561}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89440.08572319617 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, batch=0 train rmse <loss>=122.76023786226548\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, batch=0 train mse <loss>=15070.076\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, batch=0 train absolute_loss <loss>=63.9927890625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:24.757] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 66, \"duration\": 917, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, train rmse <loss>=131.51631414961415\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, train mse <loss>=17296.5408875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=32, train absolute_loss <loss>=65.193160546875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512543.8226671, \"EndTime\": 1701512544.758438, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 935.2798461914062, \"count\": 1, \"min\": 935.2798461914062, \"max\": 935.2798461914062}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512543.8231184, \"EndTime\": 1701512544.758771, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2641000.0, \"count\": 1, \"min\": 2641000, \"max\": 2641000}, \"Total Batches Seen\": {\"sum\": 2641.0, \"count\": 1, \"min\": 2641, \"max\": 2641}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=85488.65023734936 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, batch=0 train rmse <loss>=122.7500875763435\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, batch=0 train mse <loss>=15067.584\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, batch=0 train absolute_loss <loss>=63.9808828125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:25.661] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 885, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, train rmse <loss>=131.50687710154173\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, train mse <loss>=17294.058725\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=33, train absolute_loss <loss>=65.18262514648437\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512544.758499, \"EndTime\": 1701512545.661913, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 902.6942253112793, \"count\": 1, \"min\": 902.6942253112793, \"max\": 902.6942253112793}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512544.7591915, \"EndTime\": 1701512545.6623456, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2721000.0, \"count\": 1, \"min\": 2721000, \"max\": 2721000}, \"Total Batches Seen\": {\"sum\": 2721.0, \"count\": 1, \"min\": 2721, \"max\": 2721}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88568.21663009586 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, batch=0 train rmse <loss>=122.73989164081904\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, batch=0 train mse <loss>=15065.081\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, batch=0 train absolute_loss <loss>=63.9688046875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:26.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 70, \"duration\": 880, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, train rmse <loss>=131.49740173288598\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, train mse <loss>=17291.5666625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=34, train absolute_loss <loss>=65.17198110351562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512545.6619847, \"EndTime\": 1701512546.5563648, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 893.5236930847168, \"count\": 1, \"min\": 893.5236930847168, \"max\": 893.5236930847168}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512545.6627748, \"EndTime\": 1701512546.5567415, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2801000.0, \"count\": 1, \"min\": 2801000, \"max\": 2801000}, \"Total Batches Seen\": {\"sum\": 2801.0, \"count\": 1, \"min\": 2801, \"max\": 2801}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89478.31830713915 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, batch=0 train rmse <loss>=122.72965004431488\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, batch=0 train mse <loss>=15062.567\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, batch=0 train absolute_loss <loss>=63.95659375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:27.460] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 72, \"duration\": 885, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, train rmse <loss>=131.4878888909545\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, train mse <loss>=17289.064925\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=35, train absolute_loss <loss>=65.16123930664062\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512546.5564256, \"EndTime\": 1701512547.4614055, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 904.294490814209, \"count\": 1, \"min\": 904.294490814209, \"max\": 904.294490814209}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512546.557083, \"EndTime\": 1701512547.4618106, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2881000.0, \"count\": 1, \"min\": 2881000, \"max\": 2881000}, \"Total Batches Seen\": {\"sum\": 2881.0, \"count\": 1, \"min\": 2881, \"max\": 2881}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88408.4600595407 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, batch=0 train rmse <loss>=122.71936277539906\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, batch=0 train mse <loss>=15060.042\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:27 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, batch=0 train absolute_loss <loss>=63.94426953125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:28.356] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 876, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, train rmse <loss>=131.478341514867\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, train mse <loss>=17286.5542875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=36, train absolute_loss <loss>=65.15041557617188\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512547.4615161, \"EndTime\": 1701512548.3573134, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 895.0960636138916, \"count\": 1, \"min\": 895.0960636138916, \"max\": 895.0960636138916}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512547.4621878, \"EndTime\": 1701512548.357712, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2961000.0, \"count\": 1, \"min\": 2961000, \"max\": 2961000}, \"Total Batches Seen\": {\"sum\": 2961.0, \"count\": 1, \"min\": 2961, \"max\": 2961}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89304.42505527227 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, batch=0 train rmse <loss>=122.70903797194403\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, batch=0 train mse <loss>=15057.508\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:28 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, batch=0 train absolute_loss <loss>=63.931828125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:29.259] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 76, \"duration\": 885, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, train rmse <loss>=131.46875897908217\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, train mse <loss>=17284.0345875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=37, train absolute_loss <loss>=65.13950083007812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512548.3574188, \"EndTime\": 1701512549.2608025, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 902.6100635528564, \"count\": 1, \"min\": 902.6100635528564, \"max\": 902.6100635528564}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512548.3581607, \"EndTime\": 1701512549.2611127, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3041000.0, \"count\": 1, \"min\": 3041000, \"max\": 3041000}, \"Total Batches Seen\": {\"sum\": 3041.0, \"count\": 1, \"min\": 3041, \"max\": 3041}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 39.0, \"count\": 1, \"min\": 39, \"max\": 39}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88582.61977203879 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, batch=0 train rmse <loss>=122.69867154945076\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, batch=0 train mse <loss>=15054.964\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:29 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, batch=0 train absolute_loss <loss>=63.919265625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:30.189] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 78, \"duration\": 923, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, train rmse <loss>=131.45914279729652\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, train mse <loss>=17281.506225\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=38, train absolute_loss <loss>=65.1285005859375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512549.2609024, \"EndTime\": 1701512550.1903768, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 928.778886795044, \"count\": 1, \"min\": 928.778886795044, \"max\": 928.778886795044}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512549.2615554, \"EndTime\": 1701512550.1906989, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3121000.0, \"count\": 1, \"min\": 3121000, \"max\": 3121000}, \"Total Batches Seen\": {\"sum\": 3121.0, \"count\": 1, \"min\": 3121, \"max\": 3121}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86086.29505047377 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, batch=0 train rmse <loss>=122.68826349736962\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, batch=0 train mse <loss>=15052.41\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:30 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, batch=0 train absolute_loss <loss>=63.9066015625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:31.083] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 872, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, train rmse <loss>=131.44949453116965\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, train mse <loss>=17278.9696125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=39, train absolute_loss <loss>=65.11743403320312\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512550.1904707, \"EndTime\": 1701512551.0844812, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 893.3885097503662, \"count\": 1, \"min\": 893.3885097503662, \"max\": 893.3885097503662}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512550.1910691, \"EndTime\": 1701512551.0848184, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3201000.0, \"count\": 1, \"min\": 3201000, \"max\": 3201000}, \"Total Batches Seen\": {\"sum\": 3201.0, \"count\": 1, \"min\": 3201, \"max\": 3201}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 41.0, \"count\": 1, \"min\": 41, \"max\": 41}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89498.79559334664 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, batch=0 train rmse <loss>=122.67782195653785\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, batch=0 train mse <loss>=15049.848\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, batch=0 train absolute_loss <loss>=63.89384375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:31.985] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 82, \"duration\": 884, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, train rmse <loss>=131.43981388833444\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, train mse <loss>=17276.424675\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #quality_metric: host=algo-1, epoch=40, train absolute_loss <loss>=65.10629345703126\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512551.084555, \"EndTime\": 1701512551.9862125, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 900.9854793548584, \"count\": 1, \"min\": 900.9854793548584, \"max\": 900.9854793548584}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512551.0851853, \"EndTime\": 1701512551.9866104, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3281000.0, \"count\": 1, \"min\": 3281000, \"max\": 3281000}, \"Total Batches Seen\": {\"sum\": 3281.0, \"count\": 1, \"min\": 3281, \"max\": 3281}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:31 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88734.11260947496 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, batch=0 train rmse <loss>=122.6673387662747\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, batch=0 train mse <loss>=15047.276\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, batch=0 train absolute_loss <loss>=63.880984375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:32.894] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 84, \"duration\": 887, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, train rmse <loss>=131.43010290644986\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, train mse <loss>=17273.87195\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=41, train absolute_loss <loss>=65.095083984375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512551.9863002, \"EndTime\": 1701512552.8950746, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 908.0333709716797, \"count\": 1, \"min\": 908.0333709716797, \"max\": 908.0333709716797}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512551.9870117, \"EndTime\": 1701512552.8953345, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3361000.0, \"count\": 1, \"min\": 3361000, \"max\": 3361000}, \"Total Batches Seen\": {\"sum\": 3361.0, \"count\": 1, \"min\": 3361, \"max\": 3361}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 43.0, \"count\": 1, \"min\": 43, \"max\": 43}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88057.49565544176 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, batch=0 train rmse <loss>=122.65681799231545\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, batch=0 train mse <loss>=15044.695\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:32 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, batch=0 train absolute_loss <loss>=63.86805078125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:33.801] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 889, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, train rmse <loss>=131.42036248237943\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, train mse <loss>=17271.311675\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=42, train absolute_loss <loss>=65.08380678710938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512552.8951516, \"EndTime\": 1701512553.8023217, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 906.6271781921387, \"count\": 1, \"min\": 906.6271781921387, \"max\": 906.6271781921387}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512552.895668, \"EndTime\": 1701512553.8026965, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3441000.0, \"count\": 1, \"min\": 3441000, \"max\": 3441000}, \"Total Batches Seen\": {\"sum\": 3441.0, \"count\": 1, \"min\": 3441, \"max\": 3441}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88188.71943238672 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, batch=0 train rmse <loss>=122.64626777851824\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, batch=0 train mse <loss>=15042.107\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:33 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, batch=0 train absolute_loss <loss>=63.855015625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:34.729] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 88, \"duration\": 910, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, train rmse <loss>=131.41059208640678\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, train mse <loss>=17268.7437125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=43, train absolute_loss <loss>=65.07245854492187\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512553.8023834, \"EndTime\": 1701512554.7303662, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 927.2365570068359, \"count\": 1, \"min\": 927.2365570068359, \"max\": 927.2365570068359}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512553.8030214, \"EndTime\": 1701512554.7306566, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3521000.0, \"count\": 1, \"min\": 3521000, \"max\": 3521000}, \"Total Batches Seen\": {\"sum\": 3521.0, \"count\": 1, \"min\": 3521, \"max\": 3521}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 45.0, \"count\": 1, \"min\": 45, \"max\": 45}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86225.06420413342 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, batch=0 train rmse <loss>=122.63568404016834\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, batch=0 train mse <loss>=15039.511\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:34 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, batch=0 train absolute_loss <loss>=63.84190625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:35.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 90, \"duration\": 880, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, train rmse <loss>=131.4007941376307\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, train mse <loss>=17266.1687\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=44, train absolute_loss <loss>=65.06105327148437\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512554.7304282, \"EndTime\": 1701512555.628798, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 897.7293968200684, \"count\": 1, \"min\": 897.7293968200684, \"max\": 897.7293968200684}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512554.7310448, \"EndTime\": 1701512555.6291428, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3601000.0, \"count\": 1, \"min\": 3601000, \"max\": 3601000}, \"Total Batches Seen\": {\"sum\": 3601.0, \"count\": 1, \"min\": 3601, \"max\": 3601}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89061.41665655845 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, batch=0 train rmse <loss>=122.62506269111547\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, batch=0 train mse <loss>=15036.906\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:35 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, batch=0 train absolute_loss <loss>=63.82872265625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:36.535] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 890, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, train rmse <loss>=131.39096820177556\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, train mse <loss>=17263.586525\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=45, train absolute_loss <loss>=65.04958408203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512555.6288693, \"EndTime\": 1701512556.5363896, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 906.8548679351807, \"count\": 1, \"min\": 906.8548679351807, \"max\": 906.8548679351807}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512555.629509, \"EndTime\": 1701512556.5367165, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3681000.0, \"count\": 1, \"min\": 3681000, \"max\": 3681000}, \"Total Batches Seen\": {\"sum\": 3681.0, \"count\": 1, \"min\": 3681, \"max\": 3681}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88171.13082998936 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, batch=0 train rmse <loss>=122.61441187723408\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, batch=0 train mse <loss>=15034.294\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:36 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, batch=0 train absolute_loss <loss>=63.8154609375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:37.438] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 94, \"duration\": 884, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, train rmse <loss>=131.3811153667071\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, train mse <loss>=17260.997475\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=46, train absolute_loss <loss>=65.03805732421876\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512556.5364633, \"EndTime\": 1701512557.4393773, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 902.233362197876, \"count\": 1, \"min\": 902.233362197876, \"max\": 902.233362197876}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512556.5371141, \"EndTime\": 1701512557.4396298, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3761000.0, \"count\": 1, \"min\": 3761000, \"max\": 3761000}, \"Total Batches Seen\": {\"sum\": 3761.0, \"count\": 1, \"min\": 3761, \"max\": 3761}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88626.93411649296 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, batch=0 train rmse <loss>=122.60372751266578\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, batch=0 train mse <loss>=15031.674\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:37 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, batch=0 train absolute_loss <loss>=63.80213671875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:38.346] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 96, \"duration\": 891, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, train rmse <loss>=131.371235198197\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, train mse <loss>=17258.4014375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=47, train absolute_loss <loss>=65.02647236328124\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512557.4394512, \"EndTime\": 1701512558.3476467, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 907.7024459838867, \"count\": 1, \"min\": 907.7024459838867, \"max\": 907.7024459838867}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512557.4399147, \"EndTime\": 1701512558.3478932, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3841000.0, \"count\": 1, \"min\": 3841000, \"max\": 3841000}, \"Total Batches Seen\": {\"sum\": 3841.0, \"count\": 1, \"min\": 3841, \"max\": 3841}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 49.0, \"count\": 1, \"min\": 49, \"max\": 49}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88094.16192692537 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, batch=0 train rmse <loss>=122.59301366717436\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, batch=0 train mse <loss>=15029.047\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:38 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, batch=0 train absolute_loss <loss>=63.78875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:39.342] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 974, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, train rmse <loss>=131.36132906985983\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, train mse <loss>=17255.798775\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=48, train absolute_loss <loss>=65.01482875976562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512558.3477228, \"EndTime\": 1701512559.343151, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 994.9507713317871, \"count\": 1, \"min\": 994.9507713317871, \"max\": 994.9507713317871}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512558.3481708, \"EndTime\": 1701512559.3434665, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3921000.0, \"count\": 1, \"min\": 3921000, \"max\": 3921000}, \"Total Batches Seen\": {\"sum\": 3921.0, \"count\": 1, \"min\": 3921, \"max\": 3921}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=80364.44954749175 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, batch=0 train rmse <loss>=122.58227033302981\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, batch=0 train mse <loss>=15026.413\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:39 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, batch=0 train absolute_loss <loss>=63.775296875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:40.239] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 100, \"duration\": 869, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, train rmse <loss>=131.3513984984553\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, train mse <loss>=17253.1898875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=49, train absolute_loss <loss>=65.00312294921875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512559.3432589, \"EndTime\": 1701512560.2408118, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 896.8906402587891, \"count\": 1, \"min\": 896.8906402587891, \"max\": 896.8906402587891}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512559.3438826, \"EndTime\": 1701512560.2411566, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4001000.0, \"count\": 1, \"min\": 4001000, \"max\": 4001000}, \"Total Batches Seen\": {\"sum\": 4001.0, \"count\": 1, \"min\": 4001, \"max\": 4001}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 51.0, \"count\": 1, \"min\": 51, \"max\": 51}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89140.91463234827 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, batch=0 train rmse <loss>=122.57149750247812\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, batch=0 train mse <loss>=15023.772\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:40 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, batch=0 train absolute_loss <loss>=63.7617890625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:41.218] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 102, \"duration\": 961, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, train rmse <loss>=131.34144186051864\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, train mse <loss>=17250.57435\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=50, train absolute_loss <loss>=64.99136655273438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512560.2408926, \"EndTime\": 1701512561.2198064, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 978.3425331115723, \"count\": 1, \"min\": 978.3425331115723, \"max\": 978.3425331115723}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512560.2414374, \"EndTime\": 1701512561.2201803, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4081000.0, \"count\": 1, \"min\": 4081000, \"max\": 4081000}, \"Total Batches Seen\": {\"sum\": 4081.0, \"count\": 1, \"min\": 4081, \"max\": 4081}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=81727.53325542437 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, batch=0 train rmse <loss>=122.56069924735253\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, batch=0 train mse <loss>=15021.125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:41 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, batch=0 train absolute_loss <loss>=63.74823046875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:42.096] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 859, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, train rmse <loss>=131.33146076816476\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, train mse <loss>=17247.9525875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=51, train absolute_loss <loss>=64.97955693359376\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512561.2198665, \"EndTime\": 1701512562.0971227, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 876.5180110931396, \"count\": 1, \"min\": 876.5180110931396, \"max\": 876.5180110931396}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512561.220567, \"EndTime\": 1701512562.097515, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4161000.0, \"count\": 1, \"min\": 4161000, \"max\": 4161000}, \"Total Batches Seen\": {\"sum\": 4161.0, \"count\": 1, \"min\": 4161, \"max\": 4161}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 53.0, \"count\": 1, \"min\": 53, \"max\": 53}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=91211.30555475516 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, batch=0 train rmse <loss>=122.54986740098906\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, batch=0 train mse <loss>=15018.47\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, batch=0 train absolute_loss <loss>=63.734625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:42.986] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 106, \"duration\": 872, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, train rmse <loss>=131.3214551682245\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, train mse <loss>=17245.3245875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #quality_metric: host=algo-1, epoch=52, train absolute_loss <loss>=64.96770063476562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512562.0972466, \"EndTime\": 1701512562.986989, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.9901638031006, \"count\": 1, \"min\": 888.9901638031006, \"max\": 888.9901638031006}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512562.0979686, \"EndTime\": 1701512562.9872353, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4241000.0, \"count\": 1, \"min\": 4241000, \"max\": 4241000}, \"Total Batches Seen\": {\"sum\": 4241.0, \"count\": 1, \"min\": 4241, \"max\": 4241}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:42 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89946.63960313976 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, batch=0 train rmse <loss>=122.5390101151466\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, batch=0 train mse <loss>=15015.809\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, batch=0 train absolute_loss <loss>=63.72098046875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:43.891] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 108, \"duration\": 883, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, train rmse <loss>=131.31142624501496\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, train mse <loss>=17242.6906625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=53, train absolute_loss <loss>=64.95578486328125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512562.9870634, \"EndTime\": 1701512563.8923278, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 904.7162532806396, \"count\": 1, \"min\": 904.7162532806396, \"max\": 904.7162532806396}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512562.9875193, \"EndTime\": 1701512563.8927567, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4321000.0, \"count\": 1, \"min\": 4321000, \"max\": 4321000}, \"Total Batches Seen\": {\"sum\": 4321.0, \"count\": 1, \"min\": 4321, \"max\": 4321}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88361.8972730459 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, batch=0 train rmse <loss>=122.5281233023668\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, batch=0 train mse <loss>=15013.141\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:43 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, batch=0 train absolute_loss <loss>=63.70728515625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:44.806] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 892, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, train rmse <loss>=131.30137385039046\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, train mse <loss>=17240.050775\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=54, train absolute_loss <loss>=64.94382353515626\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512563.892432, \"EndTime\": 1701512564.8075695, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 914.4084453582764, \"count\": 1, \"min\": 914.4084453582764, \"max\": 914.4084453582764}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512563.8931348, \"EndTime\": 1701512564.807863, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4401000.0, \"count\": 1, \"min\": 4401000, \"max\": 4401000}, \"Total Batches Seen\": {\"sum\": 4401.0, \"count\": 1, \"min\": 4401, \"max\": 4401}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87443.40904506689 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, batch=0 train rmse <loss>=122.51721103583773\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, batch=0 train mse <loss>=15010.467\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:44 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, batch=0 train absolute_loss <loss>=63.6935625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:45.695] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 112, \"duration\": 871, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, train rmse <loss>=131.29129755052313\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, train mse <loss>=17237.4048125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=55, train absolute_loss <loss>=64.93181821289062\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512564.8076434, \"EndTime\": 1701512565.6967525, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.4832859039307, \"count\": 1, \"min\": 888.4832859039307, \"max\": 888.4832859039307}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512564.8082392, \"EndTime\": 1701512565.6970727, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4481000.0, \"count\": 1, \"min\": 4481000, \"max\": 4481000}, \"Total Batches Seen\": {\"sum\": 4481.0, \"count\": 1, \"min\": 4481, \"max\": 4481}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 57.0, \"count\": 1, \"min\": 57, \"max\": 57}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89986.7062537998 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, batch=0 train rmse <loss>=122.50627330875754\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, batch=0 train mse <loss>=15007.787\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:45 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, batch=0 train absolute_loss <loss>=63.67981640625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:46.595] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 114, \"duration\": 881, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, train rmse <loss>=131.2811987205327\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, train mse <loss>=17234.7531375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=56, train absolute_loss <loss>=64.91976811523438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512565.6968598, \"EndTime\": 1701512566.5962057, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 898.6978530883789, \"count\": 1, \"min\": 898.6978530883789, \"max\": 898.6978530883789}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512565.6974764, \"EndTime\": 1701512566.5965545, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4561000.0, \"count\": 1, \"min\": 4561000, \"max\": 4561000}, \"Total Batches Seen\": {\"sum\": 4561.0, \"count\": 1, \"min\": 4561, \"max\": 4561}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88959.27157945081 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, batch=0 train rmse <loss>=122.49531011430601\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, batch=0 train mse <loss>=15005.101\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:46 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, batch=0 train absolute_loss <loss>=63.66603125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:47.505] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 892, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, train rmse <loss>=131.27107735521943\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, train mse <loss>=17232.09575\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=57, train absolute_loss <loss>=64.90766743164062\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512566.5963147, \"EndTime\": 1701512567.5066175, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 909.5602035522461, \"count\": 1, \"min\": 909.5602035522461, \"max\": 909.5602035522461}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512566.596976, \"EndTime\": 1701512567.506997, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4641000.0, \"count\": 1, \"min\": 4641000, \"max\": 4641000}, \"Total Batches Seen\": {\"sum\": 4641.0, \"count\": 1, \"min\": 4641, \"max\": 4641}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 59.0, \"count\": 1, \"min\": 59, \"max\": 59}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87893.10466856365 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, batch=0 train rmse <loss>=122.48432144564462\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, batch=0 train mse <loss>=15002.409\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:47 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, batch=0 train absolute_loss <loss>=63.65221875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:48.397] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 118, \"duration\": 873, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, train rmse <loss>=131.26093430644167\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, train mse <loss>=17229.432875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=58, train absolute_loss <loss>=64.89551547851562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512567.5066886, \"EndTime\": 1701512568.3984141, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 890.9749984741211, \"count\": 1, \"min\": 890.9749984741211, \"max\": 890.9749984741211}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512567.50739, \"EndTime\": 1701512568.3988318, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4721000.0, \"count\": 1, \"min\": 4721000, \"max\": 4721000}, \"Total Batches Seen\": {\"sum\": 4721.0, \"count\": 1, \"min\": 4721, \"max\": 4721}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89709.98493980333 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, batch=0 train rmse <loss>=122.47330321339422\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, batch=0 train mse <loss>=14999.71\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:48 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, batch=0 train absolute_loss <loss>=63.638375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:49.306] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 120, \"duration\": 891, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, train rmse <loss>=131.25076871203458\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, train mse <loss>=17226.7642875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=59, train absolute_loss <loss>=64.88332065429688\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512568.3985295, \"EndTime\": 1701512569.3074856, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 908.1869125366211, \"count\": 1, \"min\": 908.1869125366211, \"max\": 908.1869125366211}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512568.3992677, \"EndTime\": 1701512569.3077986, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4801000.0, \"count\": 1, \"min\": 4801000, \"max\": 4801000}, \"Total Batches Seen\": {\"sum\": 4801.0, \"count\": 1, \"min\": 4801, \"max\": 4801}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 61.0, \"count\": 1, \"min\": 61, \"max\": 61}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88037.34914280527 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, batch=0 train rmse <loss>=122.46226357535615\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, batch=0 train mse <loss>=14997.006\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:49 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, batch=0 train absolute_loss <loss>=63.62451953125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:50.197] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 872, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, train rmse <loss>=131.24058113822872\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, train mse <loss>=17224.0901375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=60, train absolute_loss <loss>=64.87108461914063\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512569.3075469, \"EndTime\": 1701512570.1980119, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 889.7929191589355, \"count\": 1, \"min\": 889.7929191589355, \"max\": 889.7929191589355}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512569.308195, \"EndTime\": 1701512570.1983924, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4881000.0, \"count\": 1, \"min\": 4881000, \"max\": 4881000}, \"Total Batches Seen\": {\"sum\": 4881.0, \"count\": 1, \"min\": 4881, \"max\": 4881}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89852.31890119906 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, batch=0 train rmse <loss>=122.45119844248157\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, batch=0 train mse <loss>=14994.296\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:50 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, batch=0 train absolute_loss <loss>=63.6106484375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:51.099] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 124, \"duration\": 884, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, train rmse <loss>=131.2303727229333\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, train mse <loss>=17221.410725\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=61, train absolute_loss <loss>=64.8588091796875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512570.198081, \"EndTime\": 1701512571.0998096, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 900.9354114532471, \"count\": 1, \"min\": 900.9354114532471, \"max\": 900.9354114532471}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512570.198846, \"EndTime\": 1701512571.10016, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4961000.0, \"count\": 1, \"min\": 4961000, \"max\": 4961000}, \"Total Batches Seen\": {\"sum\": 4961.0, \"count\": 1, \"min\": 4961, \"max\": 4961}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 63.0, \"count\": 1, \"min\": 63, \"max\": 63}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88743.05389707748 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, batch=0 train rmse <loss>=122.44010780785845\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, batch=0 train mse <loss>=14991.58\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, batch=0 train absolute_loss <loss>=63.59675390625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:51.999] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 126, \"duration\": 882, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, train rmse <loss>=131.22014222290724\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, train mse <loss>=17218.725725\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #quality_metric: host=algo-1, epoch=62, train absolute_loss <loss>=64.84648813476562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512571.099923, \"EndTime\": 1701512571.9997451, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 899.1537094116211, \"count\": 1, \"min\": 899.1537094116211, \"max\": 899.1537094116211}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:51 INFO 139702296962880] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512571.1005237, \"EndTime\": 1701512571.999986, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5041000.0, \"count\": 1, \"min\": 5041000, \"max\": 5041000}, \"Total Batches Seen\": {\"sum\": 5041.0, \"count\": 1, \"min\": 5041, \"max\": 5041}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88925.11030027541 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, batch=0 train rmse <loss>=122.42899574855623\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, batch=0 train mse <loss>=14988.859\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, batch=0 train absolute_loss <loss>=63.58283984375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:52.898] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 882, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, train rmse <loss>=131.20989130016076\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, train mse <loss>=17216.035575\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=63, train absolute_loss <loss>=64.83412451171876\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512571.9998195, \"EndTime\": 1701512572.8994062, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 899.0812301635742, \"count\": 1, \"min\": 899.0812301635742, \"max\": 899.0812301635742}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512572.0003006, \"EndTime\": 1701512572.8997962, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5121000.0, \"count\": 1, \"min\": 5121000, \"max\": 5121000}, \"Total Batches Seen\": {\"sum\": 5121.0, \"count\": 1, \"min\": 5121, \"max\": 5121}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 65.0, \"count\": 1, \"min\": 65, \"max\": 65}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88922.82438829575 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, batch=0 train rmse <loss>=122.41786225874066\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, batch=0 train mse <loss>=14986.133\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:52 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, batch=0 train absolute_loss <loss>=63.56891796875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:53.790] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 130, \"duration\": 873, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, train rmse <loss>=131.19961852078686\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, train mse <loss>=17213.3399\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=64, train absolute_loss <loss>=64.82172333984374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512572.8994653, \"EndTime\": 1701512573.791186, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 890.9602165222168, \"count\": 1, \"min\": 890.9602165222168, \"max\": 890.9602165222168}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512572.9001956, \"EndTime\": 1701512573.7915761, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5201000.0, \"count\": 1, \"min\": 5201000, \"max\": 5201000}, \"Total Batches Seen\": {\"sum\": 5201.0, \"count\": 1, \"min\": 5201, \"max\": 5201}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89735.91973988312 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, batch=0 train rmse <loss>=122.40669916307685\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, batch=0 train mse <loss>=14983.4\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:53 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, batch=0 train absolute_loss <loss>=63.5549765625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:54.694] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 132, \"duration\": 887, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, train rmse <loss>=131.18932626170468\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, train mse <loss>=17210.639325\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=65, train absolute_loss <loss>=64.80927763671875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512573.7912605, \"EndTime\": 1701512574.6957037, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 903.712272644043, \"count\": 1, \"min\": 903.712272644043, \"max\": 903.712272644043}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512573.7919648, \"EndTime\": 1701512574.696026, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5281000.0, \"count\": 1, \"min\": 5281000, \"max\": 5281000}, \"Total Batches Seen\": {\"sum\": 5281.0, \"count\": 1, \"min\": 5281, \"max\": 5281}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 67.0, \"count\": 1, \"min\": 67, \"max\": 67}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88474.38365494116 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, batch=0 train rmse <loss>=122.39551870881549\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, batch=0 train mse <loss>=14980.663\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:54 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, batch=0 train absolute_loss <loss>=63.54102734375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:55.611] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 899, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, train rmse <loss>=131.17901261253647\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, train mse <loss>=17207.93335\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=66, train absolute_loss <loss>=64.79680068359374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512574.6958127, \"EndTime\": 1701512575.612511, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 916.069746017456, \"count\": 1, \"min\": 916.069746017456, \"max\": 916.069746017456}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512574.696413, \"EndTime\": 1701512575.6128445, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5361000.0, \"count\": 1, \"min\": 5361000, \"max\": 5361000}, \"Total Batches Seen\": {\"sum\": 5361.0, \"count\": 1, \"min\": 5361, \"max\": 5361}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87275.44367393831 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, batch=0 train rmse <loss>=122.38430863472654\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, batch=0 train mse <loss>=14977.919\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:55 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, batch=0 train absolute_loss <loss>=63.5270625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:56.517] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 136, \"duration\": 884, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, train rmse <loss>=131.16867899769366\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, train mse <loss>=17205.22235\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=67, train absolute_loss <loss>=64.78428569335938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512575.6126318, \"EndTime\": 1701512576.5186412, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 905.3349494934082, \"count\": 1, \"min\": 905.3349494934082, \"max\": 905.3349494934082}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512575.613263, \"EndTime\": 1701512576.519007, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5441000.0, \"count\": 1, \"min\": 5441000, \"max\": 5441000}, \"Total Batches Seen\": {\"sum\": 5441.0, \"count\": 1, \"min\": 5441, \"max\": 5441}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 69.0, \"count\": 1, \"min\": 69, \"max\": 69}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88306.38874838346 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, batch=0 train rmse <loss>=122.37308119026831\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, batch=0 train mse <loss>=14975.171\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:56 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, batch=0 train absolute_loss <loss>=63.51309375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:57.410] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 138, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, train rmse <loss>=131.1583256507188\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, train mse <loss>=17202.5063875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=68, train absolute_loss <loss>=64.77172763671875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512576.5187016, \"EndTime\": 1701512577.4112017, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 891.7663097381592, \"count\": 1, \"min\": 891.7663097381592, \"max\": 891.7663097381592}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512576.519408, \"EndTime\": 1701512577.4115224, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5521000.0, \"count\": 1, \"min\": 5521000, \"max\": 5521000}, \"Total Batches Seen\": {\"sum\": 5521.0, \"count\": 1, \"min\": 5521, \"max\": 5521}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89657.24993540454 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, batch=0 train rmse <loss>=122.36182819817624\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, batch=0 train mse <loss>=14972.417\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:57 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, batch=0 train absolute_loss <loss>=63.4991015625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:58.299] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, train rmse <loss>=131.14795199506548\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, train mse <loss>=17199.7853125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=69, train absolute_loss <loss>=64.75912895507813\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512577.4113083, \"EndTime\": 1701512578.3006365, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.667106628418, \"count\": 1, \"min\": 888.667106628418, \"max\": 888.667106628418}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512577.4119427, \"EndTime\": 1701512578.3009913, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5601000.0, \"count\": 1, \"min\": 5601000, \"max\": 5601000}, \"Total Batches Seen\": {\"sum\": 5601.0, \"count\": 1, \"min\": 5601, \"max\": 5601}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 71.0, \"count\": 1, \"min\": 71, \"max\": 71}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89963.97894450666 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, batch=0 train rmse <loss>=122.35054965140124\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, batch=0 train mse <loss>=14969.657\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:58 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, batch=0 train absolute_loss <loss>=63.48509765625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:22:59.211] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 142, \"duration\": 894, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, train rmse <loss>=131.137558073574\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, train mse <loss>=17197.0591375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=70, train absolute_loss <loss>=64.74650366210938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512578.3007774, \"EndTime\": 1701512579.2125268, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 911.1316204071045, \"count\": 1, \"min\": 911.1316204071045, \"max\": 911.1316204071045}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512578.3013651, \"EndTime\": 1701512579.2127802, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5681000.0, \"count\": 1, \"min\": 5681000, \"max\": 5681000}, \"Total Batches Seen\": {\"sum\": 5681.0, \"count\": 1, \"min\": 5681, \"max\": 5681}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87759.59088247287 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, batch=0 train rmse <loss>=122.33925371686718\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, batch=0 train mse <loss>=14966.893\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:22:59 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, batch=0 train absolute_loss <loss>=63.47109375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:00.116] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 144, \"duration\": 886, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, train rmse <loss>=131.12714531133514\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, train mse <loss>=17194.3282375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=71, train absolute_loss <loss>=64.73384228515626\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512579.2126014, \"EndTime\": 1701512580.116961, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 903.850793838501, \"count\": 1, \"min\": 903.850793838501, \"max\": 903.850793838501}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512579.2130578, \"EndTime\": 1701512580.1173313, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5761000.0, \"count\": 1, \"min\": 5761000, \"max\": 5761000}, \"Total Batches Seen\": {\"sum\": 5761.0, \"count\": 1, \"min\": 5761, \"max\": 5761}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 73.0, \"count\": 1, \"min\": 73, \"max\": 73}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88451.52771594019 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, batch=0 train rmse <loss>=122.32793630238352\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, batch=0 train mse <loss>=14964.124\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:00 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, batch=0 train absolute_loss <loss>=63.4570703125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:01.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 882, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, train rmse <loss>=131.11671265517603\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, train mse <loss>=17191.5923375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=72, train absolute_loss <loss>=64.72113369140625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512580.1170914, \"EndTime\": 1701512581.01744, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 899.6469974517822, \"count\": 1, \"min\": 899.6469974517822, \"max\": 899.6469974517822}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512580.1177652, \"EndTime\": 1701512581.0178154, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5841000.0, \"count\": 1, \"min\": 5841000, \"max\": 5841000}, \"Total Batches Seen\": {\"sum\": 5841.0, \"count\": 1, \"min\": 5841, \"max\": 5841}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88866.70325037025 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, batch=0 train rmse <loss>=122.31659740198793\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, batch=0 train mse <loss>=14961.35\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:01 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, batch=0 train absolute_loss <loss>=63.44302734375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:02.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 148, \"duration\": 1420, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, train rmse <loss>=131.10625995733383\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, train mse <loss>=17188.8514\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=73, train absolute_loss <loss>=64.70839868164063\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512581.017551, \"EndTime\": 1701512582.4562197, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1437.9513263702393, \"count\": 1, \"min\": 1437.9513263702393, \"max\": 1437.9513263702393}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512581.0182376, \"EndTime\": 1701512582.4568968, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5921000.0, \"count\": 1, \"min\": 5921000, \"max\": 5921000}, \"Total Batches Seen\": {\"sum\": 5921.0, \"count\": 1, \"min\": 5921, \"max\": 5921}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 75.0, \"count\": 1, \"min\": 75, \"max\": 75}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=55598.30287677829 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, batch=0 train rmse <loss>=122.3052288334395\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, batch=0 train mse <loss>=14958.569\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:02 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, batch=0 train absolute_loss <loss>=63.428984375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:03.845] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 150, \"duration\": 1359, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, train rmse <loss>=131.09578878629168\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, train mse <loss>=17186.1058375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=74, train absolute_loss <loss>=64.69563305664063\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512582.456361, \"EndTime\": 1701512583.8459656, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1388.6003494262695, \"count\": 1, \"min\": 1388.6003494262695, \"max\": 1388.6003494262695}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512582.4573364, \"EndTime\": 1701512583.8462248, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6001000.0, \"count\": 1, \"min\": 6001000, \"max\": 6001000}, \"Total Batches Seen\": {\"sum\": 6001.0, \"count\": 1, \"min\": 6001, \"max\": 6001}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=57593.62533935844 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, batch=0 train rmse <loss>=122.29384694251792\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, batch=0 train mse <loss>=14955.785\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:03 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, batch=0 train absolute_loss <loss>=63.414921875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:04.751] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 889, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, train rmse <loss>=131.08529861315492\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, train mse <loss>=17183.3555125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=75, train absolute_loss <loss>=64.6828296875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512583.8460486, \"EndTime\": 1701512584.7525582, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 906.0163497924805, \"count\": 1, \"min\": 906.0163497924805, \"max\": 906.0163497924805}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512583.8465128, \"EndTime\": 1701512584.7528064, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6081000.0, \"count\": 1, \"min\": 6081000, \"max\": 6081000}, \"Total Batches Seen\": {\"sum\": 6081.0, \"count\": 1, \"min\": 6081, \"max\": 6081}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 77.0, \"count\": 1, \"min\": 77, \"max\": 77}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88258.70298674822 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, batch=0 train rmse <loss>=122.2824394588201\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, batch=0 train mse <loss>=14952.995\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:04 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, batch=0 train absolute_loss <loss>=63.40083984375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:05.643] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 154, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, train rmse <loss>=131.07478914726508\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, train mse <loss>=17180.60035\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=76, train absolute_loss <loss>=64.6699873046875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512584.7526336, \"EndTime\": 1701512585.644894, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 891.7415142059326, \"count\": 1, \"min\": 891.7415142059326, \"max\": 891.7415142059326}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512584.753127, \"EndTime\": 1701512585.645279, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6161000.0, \"count\": 1, \"min\": 6161000, \"max\": 6161000}, \"Total Batches Seen\": {\"sum\": 6161.0, \"count\": 1, \"min\": 6161, \"max\": 6161}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89659.21440044077 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, batch=0 train rmse <loss>=122.27101455373632\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, batch=0 train mse <loss>=14950.201\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:05 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, batch=0 train absolute_loss <loss>=63.38674609375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:06.561] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 156, \"duration\": 899, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, train rmse <loss>=131.06426148077134\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, train mse <loss>=17177.8406375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=77, train absolute_loss <loss>=64.65711606445312\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512585.644968, \"EndTime\": 1701512586.5623758, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 916.5992736816406, \"count\": 1, \"min\": 916.5992736816406, \"max\": 916.5992736816406}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512585.6457481, \"EndTime\": 1701512586.5627491, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6241000.0, \"count\": 1, \"min\": 6241000, \"max\": 6241000}, \"Total Batches Seen\": {\"sum\": 6241.0, \"count\": 1, \"min\": 6241, \"max\": 6241}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 79.0, \"count\": 1, \"min\": 79, \"max\": 79}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87225.53127109676 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, batch=0 train rmse <loss>=122.25956404306372\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, batch=0 train mse <loss>=14947.401\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:06 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, batch=0 train absolute_loss <loss>=63.372640625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:07.442] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 863, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, train rmse <loss>=131.05371408319567\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, train mse <loss>=17175.075975\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=78, train absolute_loss <loss>=64.64422045898438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512586.5624542, \"EndTime\": 1701512587.4437616, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 880.5732727050781, \"count\": 1, \"min\": 880.5732727050781, \"max\": 880.5732727050781}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512586.5631628, \"EndTime\": 1701512587.4441257, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6321000.0, \"count\": 1, \"min\": 6321000, \"max\": 6321000}, \"Total Batches Seen\": {\"sum\": 6321.0, \"count\": 1, \"min\": 6321, \"max\": 6321}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90797.19964844071 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, batch=0 train rmse <loss>=122.24809609969392\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, batch=0 train mse <loss>=14944.597\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:07 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, batch=0 train absolute_loss <loss>=63.3585234375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:08.350] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 160, \"duration\": 889, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, train rmse <loss>=131.04314857137706\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, train mse <loss>=17172.3067875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=79, train absolute_loss <loss>=64.63127592773438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512587.4438357, \"EndTime\": 1701512588.3513527, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 906.7642688751221, \"count\": 1, \"min\": 906.7642688751221, \"max\": 906.7642688751221}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512587.4444923, \"EndTime\": 1701512588.3516312, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6401000.0, \"count\": 1, \"min\": 6401000, \"max\": 6401000}, \"Total Batches Seen\": {\"sum\": 6401.0, \"count\": 1, \"min\": 6401, \"max\": 6401}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 81.0, \"count\": 1, \"min\": 81, \"max\": 81}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88172.42830005779 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, batch=0 train rmse <loss>=122.23660253786507\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, batch=0 train mse <loss>=14941.787\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:08 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, batch=0 train absolute_loss <loss>=63.3443671875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:09.263] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 162, \"duration\": 894, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, train rmse <loss>=131.03256427316074\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, train mse <loss>=17169.5329\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=80, train absolute_loss <loss>=64.61830502929688\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512588.35142, \"EndTime\": 1701512589.2639344, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 911.8747711181641, \"count\": 1, \"min\": 911.8747711181641, \"max\": 911.8747711181641}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512588.352032, \"EndTime\": 1701512589.264271, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6481000.0, \"count\": 1, \"min\": 6481000, \"max\": 6481000}, \"Total Batches Seen\": {\"sum\": 6481.0, \"count\": 1, \"min\": 6481, \"max\": 6481}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87676.07564074123 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, batch=0 train rmse <loss>=122.22509153197636\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, batch=0 train mse <loss>=14938.973\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:09 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, batch=0 train absolute_loss <loss>=63.33021484375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:10.168] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 887, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, train rmse <loss>=131.02196199492664\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, train mse <loss>=17166.754525\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=81, train absolute_loss <loss>=64.60529848632812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512589.2640407, \"EndTime\": 1701512590.169034, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 904.3095111846924, \"count\": 1, \"min\": 904.3095111846924, \"max\": 904.3095111846924}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512589.2646954, \"EndTime\": 1701512590.1692843, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6561000.0, \"count\": 1, \"min\": 6561000, \"max\": 6561000}, \"Total Batches Seen\": {\"sum\": 6561.0, \"count\": 1, \"min\": 6561, \"max\": 6561}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 83.0, \"count\": 1, \"min\": 83, \"max\": 83}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88419.6191317418 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, batch=0 train rmse <loss>=122.21356307709877\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, batch=0 train mse <loss>=14936.155\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:10 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, batch=0 train absolute_loss <loss>=63.31603125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:11.088] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 166, \"duration\": 902, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, train rmse <loss>=131.01134125525164\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, train mse <loss>=17163.9715375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=82, train absolute_loss <loss>=64.59226259765624\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512590.1691093, \"EndTime\": 1701512591.0896838, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 920.0441837310791, \"count\": 1, \"min\": 920.0441837310791, \"max\": 920.0441837310791}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512590.169601, \"EndTime\": 1701512591.090074, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6641000.0, \"count\": 1, \"min\": 6641000, \"max\": 6641000}, \"Total Batches Seen\": {\"sum\": 6641.0, \"count\": 1, \"min\": 6641, \"max\": 6641}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=86899.313702639 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, batch=0 train rmse <loss>=122.20200489353684\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, batch=0 train mse <loss>=14933.33\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, batch=0 train absolute_loss <loss>=63.3018359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:11.984] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 168, \"duration\": 877, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, train rmse <loss>=131.00070247903253\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, train mse <loss>=17161.18405\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #quality_metric: host=algo-1, epoch=83, train absolute_loss <loss>=64.57919702148438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512591.0897417, \"EndTime\": 1701512591.9857066, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 895.1988220214844, \"count\": 1, \"min\": 895.1988220214844, \"max\": 895.1988220214844}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512591.0904818, \"EndTime\": 1701512591.9860258, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6721000.0, \"count\": 1, \"min\": 6721000, \"max\": 6721000}, \"Total Batches Seen\": {\"sum\": 6721.0, \"count\": 1, \"min\": 6721, \"max\": 6721}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:11 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89314.456357085 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, batch=0 train rmse <loss>=122.1904374327222\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, batch=0 train mse <loss>=14930.503\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, batch=0 train absolute_loss <loss>=63.287625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:12.904] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 901, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, train rmse <loss>=130.99004590044237\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, train mse <loss>=17158.392125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=84, train absolute_loss <loss>=64.56609536132812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512591.9858036, \"EndTime\": 1701512592.905245, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 918.7865257263184, \"count\": 1, \"min\": 918.7865257263184, \"max\": 918.7865257263184}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512591.9863992, \"EndTime\": 1701512592.90566, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6801000.0, \"count\": 1, \"min\": 6801000, \"max\": 6801000}, \"Total Batches Seen\": {\"sum\": 6801.0, \"count\": 1, \"min\": 6801, \"max\": 6801}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=87012.03070924498 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, batch=0 train rmse <loss>=122.17884023021335\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, batch=0 train mse <loss>=14927.669\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:12 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, batch=0 train absolute_loss <loss>=63.273375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:13.787] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 172, \"duration\": 865, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, train rmse <loss>=130.97937094252669\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, train mse <loss>=17155.5956125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=85, train absolute_loss <loss>=64.55296396484376\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512592.9053068, \"EndTime\": 1701512593.7884142, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 882.2762966156006, \"count\": 1, \"min\": 882.2762966156006, \"max\": 882.2762966156006}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512592.906109, \"EndTime\": 1701512593.7887661, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6881000.0, \"count\": 1, \"min\": 6881000, \"max\": 6881000}, \"Total Batches Seen\": {\"sum\": 6881.0, \"count\": 1, \"min\": 6881, \"max\": 6881}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 87.0, \"count\": 1, \"min\": 87, \"max\": 87}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90619.10192802796 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, batch=0 train rmse <loss>=122.16722964854364\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, batch=0 train mse <loss>=14924.832\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:13 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, batch=0 train absolute_loss <loss>=63.25912109375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:14.796] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 174, \"duration\": 989, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, train rmse <loss>=130.9686779348406\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, train mse <loss>=17152.7946\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=86, train absolute_loss <loss>=64.5398\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512593.7884746, \"EndTime\": 1701512594.7970505, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1007.8747272491455, \"count\": 1, \"min\": 1007.8747272491455, \"max\": 1007.8747272491455}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512593.789146, \"EndTime\": 1701512594.7973666, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6961000.0, \"count\": 1, \"min\": 6961000, \"max\": 6961000}, \"Total Batches Seen\": {\"sum\": 6961.0, \"count\": 1, \"min\": 6961, \"max\": 6961}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=79334.89491701909 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, batch=0 train rmse <loss>=122.15559340447739\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, batch=0 train mse <loss>=14921.989\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:14 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, batch=0 train absolute_loss <loss>=63.244828125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:15.688] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, train rmse <loss>=130.95796768429176\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, train mse <loss>=17149.9893\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=87, train absolute_loss <loss>=64.5266056640625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512594.7971559, \"EndTime\": 1701512595.689051, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 891.268253326416, \"count\": 1, \"min\": 891.268253326416, \"max\": 891.268253326416}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512594.7977548, \"EndTime\": 1701512595.689288, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7041000.0, \"count\": 1, \"min\": 7041000, \"max\": 7041000}, \"Total Batches Seen\": {\"sum\": 7041.0, \"count\": 1, \"min\": 7041, \"max\": 7041}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 89.0, \"count\": 1, \"min\": 89, \"max\": 89}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89717.06094178974 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, batch=0 train rmse <loss>=122.14394377127341\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, batch=0 train mse <loss>=14919.143\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:15 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, batch=0 train absolute_loss <loss>=63.23052734375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:16.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 178, \"duration\": 860, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, train rmse <loss>=130.9472388502331\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, train mse <loss>=17147.1793625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=88, train absolute_loss <loss>=64.51338125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512595.6891153, \"EndTime\": 1701512596.5668755, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 877.3024082183838, \"count\": 1, \"min\": 877.3024082183838, \"max\": 877.3024082183838}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512595.689547, \"EndTime\": 1701512596.5672824, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7121000.0, \"count\": 1, \"min\": 7121000, \"max\": 7121000}, \"Total Batches Seen\": {\"sum\": 7121.0, \"count\": 1, \"min\": 7121, \"max\": 7121}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=91126.86089468512 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, batch=0 train rmse <loss>=122.13226846333446\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, batch=0 train mse <loss>=14916.291\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:16 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, batch=0 train absolute_loss <loss>=63.21619140625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:17.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 180, \"duration\": 871, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, train rmse <loss>=130.93649300328767\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, train mse <loss>=17144.3652\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=89, train absolute_loss <loss>=64.50012583007812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512596.566975, \"EndTime\": 1701512597.4561605, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 888.3934020996094, \"count\": 1, \"min\": 888.3934020996094, \"max\": 888.3934020996094}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512596.5677428, \"EndTime\": 1701512597.4565432, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7201000.0, \"count\": 1, \"min\": 7201000, \"max\": 7201000}, \"Total Batches Seen\": {\"sum\": 7201.0, \"count\": 1, \"min\": 7201, \"max\": 7201}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89996.21556221967 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, batch=0 train rmse <loss>=122.120571567611\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, batch=0 train mse <loss>=14913.434\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:17 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, batch=0 train absolute_loss <loss>=63.20184765625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:18.341] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 867, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, train rmse <loss>=130.92572904131563\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, train mse <loss>=17141.546525\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=90, train absolute_loss <loss>=64.48684306640625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512597.4562206, \"EndTime\": 1701512598.342722, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 885.7808113098145, \"count\": 1, \"min\": 885.7808113098145, \"max\": 885.7808113098145}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512597.4569104, \"EndTime\": 1701512598.3430212, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7281000.0, \"count\": 1, \"min\": 7281000, \"max\": 7281000}, \"Total Batches Seen\": {\"sum\": 7281.0, \"count\": 1, \"min\": 7281, \"max\": 7281}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90265.57709477925 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, batch=0 train rmse <loss>=122.10886126731344\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, batch=0 train mse <loss>=14910.574\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:18 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, batch=0 train absolute_loss <loss>=63.18745703125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:19.244] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 184, \"duration\": 885, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, train rmse <loss>=130.9149481533717\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, train mse <loss>=17138.72365\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=91, train absolute_loss <loss>=64.473533203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512598.3428192, \"EndTime\": 1701512599.2457113, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 902.3580551147461, \"count\": 1, \"min\": 902.3580551147461, \"max\": 902.3580551147461}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512598.3433137, \"EndTime\": 1701512599.246027, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7361000.0, \"count\": 1, \"min\": 7361000, \"max\": 7361000}, \"Total Batches Seen\": {\"sum\": 7361.0, \"count\": 1, \"min\": 7361, \"max\": 7361}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 93.0, \"count\": 1, \"min\": 93, \"max\": 93}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88606.64322091591 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, batch=0 train rmse <loss>=122.09712936838442\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, batch=0 train mse <loss>=14907.709\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:19 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, batch=0 train absolute_loss <loss>=63.17307421875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:20.132] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 186, \"duration\": 870, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, train rmse <loss>=130.90414918939734\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, train mse <loss>=17135.896275\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=92, train absolute_loss <loss>=64.46019272460937\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512599.2457962, \"EndTime\": 1701512600.1336172, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 887.1140480041504, \"count\": 1, \"min\": 887.1140480041504, \"max\": 887.1140480041504}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512599.2464788, \"EndTime\": 1701512600.134019, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7441000.0, \"count\": 1, \"min\": 7441000, \"max\": 7441000}, \"Total Batches Seen\": {\"sum\": 7441.0, \"count\": 1, \"min\": 7441, \"max\": 7441}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90122.02871598846 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, batch=0 train rmse <loss>=122.08537586459731\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, batch=0 train mse <loss>=14904.839\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:20 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, batch=0 train absolute_loss <loss>=63.15862890625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:21.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 888, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, train rmse <loss>=130.89333381612678\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, train mse <loss>=17133.0648375\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=93, train absolute_loss <loss>=64.4468095703125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512600.1336772, \"EndTime\": 1701512601.0403326, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 905.9467315673828, \"count\": 1, \"min\": 905.9467315673828, \"max\": 905.9467315673828}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512600.1343267, \"EndTime\": 1701512601.040728, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7521000.0, \"count\": 1, \"min\": 7521000, \"max\": 7521000}, \"Total Batches Seen\": {\"sum\": 7521.0, \"count\": 1, \"min\": 7521, \"max\": 7521}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 95.0, \"count\": 1, \"min\": 95, \"max\": 95}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88248.1415108085 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, batch=0 train rmse <loss>=122.07360484560125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, batch=0 train mse <loss>=14901.965\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, batch=0 train absolute_loss <loss>=63.144171875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:21.931] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 190, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, train rmse <loss>=130.88250069241494\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, train mse <loss>=17130.2289875\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=94, train absolute_loss <loss>=64.433407421875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512601.0404153, \"EndTime\": 1701512601.93227, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 891.1314010620117, \"count\": 1, \"min\": 891.1314010620117, \"max\": 891.1314010620117}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512601.041114, \"EndTime\": 1701512601.9325607, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7601000.0, \"count\": 1, \"min\": 7601000, \"max\": 7601000}, \"Total Batches Seen\": {\"sum\": 7601.0, \"count\": 1, \"min\": 7601, \"max\": 7601}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89727.90498174125 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, batch=0 train rmse <loss>=122.06181221004381\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, batch=0 train mse <loss>=14899.086\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:21 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, batch=0 train absolute_loss <loss>=63.1296953125\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:22.828] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 192, \"duration\": 879, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, train rmse <loss>=130.87165000488073\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, train mse <loss>=17127.388775\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=95, train absolute_loss <loss>=64.41998046875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512601.9323428, \"EndTime\": 1701512602.8295586, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 896.6715335845947, \"count\": 1, \"min\": 896.6715335845947, \"max\": 896.6715335845947}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512601.932849, \"EndTime\": 1701512602.8299687, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7681000.0, \"count\": 1, \"min\": 7681000, \"max\": 7681000}, \"Total Batches Seen\": {\"sum\": 7681.0, \"count\": 1, \"min\": 7681, \"max\": 7681}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 97.0, \"count\": 1, \"min\": 97, \"max\": 97}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89162.68299851034 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, batch=0 train rmse <loss>=122.05000204834083\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, batch=0 train mse <loss>=14896.203\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:22 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, batch=0 train absolute_loss <loss>=63.11518359375\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:23.716] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 870, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, train rmse <loss>=130.86078260884733\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, train mse <loss>=17124.544425\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=96, train absolute_loss <loss>=64.40651650390625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512602.829639, \"EndTime\": 1701512603.7171228, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 886.7385387420654, \"count\": 1, \"min\": 886.7385387420654, \"max\": 886.7385387420654}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512602.830356, \"EndTime\": 1701512603.7174678, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7761000.0, \"count\": 1, \"min\": 7761000, \"max\": 7761000}, \"Total Batches Seen\": {\"sum\": 7761.0, \"count\": 1, \"min\": 7761, \"max\": 7761}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90150.30947965827 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, batch=0 train rmse <loss>=122.03817025832532\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, batch=0 train mse <loss>=14893.315\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:23 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, batch=0 train absolute_loss <loss>=63.100640625\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:24.616] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 196, \"duration\": 882, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, train rmse <loss>=130.84989802250516\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, train mse <loss>=17121.6958125\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=97, train absolute_loss <loss>=64.39301772460938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512603.717232, \"EndTime\": 1701512604.6176739, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 899.6479511260986, \"count\": 1, \"min\": 899.6479511260986, \"max\": 899.6479511260986}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512603.7179418, \"EndTime\": 1701512604.6180153, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7841000.0, \"count\": 1, \"min\": 7841000, \"max\": 7841000}, \"Total Batches Seen\": {\"sum\": 7841.0, \"count\": 1, \"min\": 7841, \"max\": 7841}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 99.0, \"count\": 1, \"min\": 99, \"max\": 99}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=88863.949651649 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, batch=0 train rmse <loss>=122.02632093118271\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, batch=0 train mse <loss>=14890.423\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:24 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, batch=0 train absolute_loss <loss>=63.08607421875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:25.509] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 198, \"duration\": 874, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, train rmse <loss>=130.83899648040716\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, train mse <loss>=17118.843\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=98, train absolute_loss <loss>=64.379496484375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512604.617735, \"EndTime\": 1701512605.510785, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 892.3804759979248, \"count\": 1, \"min\": 892.3804759979248, \"max\": 892.3804759979248}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512604.618366, \"EndTime\": 1701512605.5111477, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7921000.0, \"count\": 1, \"min\": 7921000, \"max\": 7921000}, \"Total Batches Seen\": {\"sum\": 7921.0, \"count\": 1, \"min\": 7921, \"max\": 7921}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=89589.24152632427 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, batch=0 train rmse <loss>=122.01445406180368\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, batch=0 train mse <loss>=14887.527\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:25 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, batch=0 train absolute_loss <loss>=63.07148046875\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:26.397] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 869, \"num_examples\": 80, \"num_bytes\": 5341604}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, train rmse <loss>=130.82807826495045\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, train mse <loss>=17115.9860625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, epoch=99, train absolute_loss <loss>=64.36595317382813\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, train rmse <loss>=130.82807826495045\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, train mse <loss>=17115.9860625\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #quality_metric: host=algo-1, train absolute_loss <loss>=64.36595317382813\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512605.5108845, \"EndTime\": 1701512606.3989253, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 887.2866630554199, \"count\": 1, \"min\": 887.2866630554199, \"max\": 887.2866630554199}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512605.5116131, \"EndTime\": 1701512606.3992937, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8001000.0, \"count\": 1, \"min\": 8001000, \"max\": 8001000}, \"Total Batches Seen\": {\"sum\": 8001.0, \"count\": 1, \"min\": 8001, \"max\": 8001}, \"Max Records Seen Between Resets\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Max Batches Seen Between Resets\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Reset Count\": {\"sum\": 101.0, \"count\": 1, \"min\": 101, \"max\": 101}, \"Number of Records Since Last Reset\": {\"sum\": 80000.0, \"count\": 1, \"min\": 80000, \"max\": 80000}, \"Number of Batches Since Last Reset\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] #throughput_metric: host=algo-1, train throughput=90111.30700532942 records/second\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 WARNING 139702296962880] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512606.3989882, \"EndTime\": 1701512606.4224951, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 22.608041763305664, \"count\": 1, \"min\": 22.608041763305664, \"max\": 22.608041763305664}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:26 INFO 139702296962880] Saved checkpoint to \"/tmp/tmp0yu6kmm3/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:26.575] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 94783, \"num_examples\": 1, \"num_bytes\": 66736}\u001b[0m\n",
      "\u001b[34m[2023-12-02 10:23:27.272] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 697, \"num_examples\": 20, \"num_bytes\": 1335812}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512606.5755234, \"EndTime\": 1701512607.2733667, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 20000.0, \"count\": 1, \"min\": 20000, \"max\": 20000}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 20000.0, \"count\": 1, \"min\": 20000, \"max\": 20000}, \"Max Batches Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 20000.0, \"count\": 1, \"min\": 20000, \"max\": 20000}, \"Number of Batches Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}}}\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #test_score (algo-1) : ('rmse', 122.809889463349)\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #test_score (algo-1) : ('mse', 15082.26895)\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #test_score (algo-1) : ('absolute_loss', 64.0142583984375)\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #quality_metric: host=algo-1, test rmse <loss>=122.809889463349\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #quality_metric: host=algo-1, test mse <loss>=15082.26895\u001b[0m\n",
      "\u001b[34m[12/02/2023 10:23:27 INFO 139702296962880] #quality_metric: host=algo-1, test absolute_loss <loss>=64.0142583984375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1701512606.422637, \"EndTime\": 1701512607.2747765, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 17.115354537963867, \"count\": 1, \"min\": 17.115354537963867, \"max\": 17.115354537963867}, \"totaltime\": {\"sum\": 95514.64128494263, \"count\": 1, \"min\": 95514.64128494263, \"max\": 95514.64128494263}}}\u001b[0m\n",
      "\n",
      "2023-12-02 10:23:30 Uploading - Uploading generated training model\n",
      "2023-12-02 10:23:46 Completed - Training job completed\n",
      "Training seconds: 258\n",
      "Billable seconds: 258\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": train_data,\n",
    "    \"test\": test_data\n",
    "}\n",
    "\n",
    "# train\n",
    "fm.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebdae20c-43b6-43e9-9734-51d44292a891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: factorization-machines-2023-12-02-10-24-26-357\n",
      "INFO:sagemaker:Creating endpoint-config with name sg-matrix-factorization\n",
      "INFO:sagemaker:Creating endpoint with name sg-matrix-factorization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!Endpoints name: sg-matrix-factorization\n"
     ]
    }
   ],
   "source": [
    "# save endpoint\n",
    "endpoint_name = 'sg-matrix-factorization'\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "endpoint_name = fm_predictor.endpoint_name\n",
    "print(f\"Endpoints name: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f282a85-c8e2-4f3b-b627-7f9bd9c21a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, X_test_OH.astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "# make SageMaker client\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# predict\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-recordio-protobuf',\n",
    "    Body=buf.read()\n",
    ")\n",
    "\n",
    "# result\n",
    "predictions = json.loads(response['Body'].read().decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e2140e9-33e3-4244-934a-d98b2016ab2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34.94242859 55.3748703  45.21507263 ... 55.28605652 14.54455566\n",
      " 43.73901367]\n",
      "2507\n",
      "20000\n",
      "평균 차이: 64.01425802650452\n"
     ]
    }
   ],
   "source": [
    "# predict result change list\n",
    "predictions_list = [pred['score'] for pred in predictions[\"predictions\"]]\n",
    "\n",
    "\n",
    "predictions_array = np.array(predictions_list)\n",
    "Y_test_array = Y_test_OH.values\n",
    "\n",
    "# mae\n",
    "diff = np.abs(predictions_array - Y_test_array)\n",
    "mean_difference = np.mean(np.abs(predictions_array - Y_test_array))\n",
    "print(diff)\n",
    "print(len(sum(np.where(diff < 20))))\n",
    "print(len(diff))\n",
    "print(\"평균 차이:\", mean_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13b637f5-472e-42ca-8288-0fabd970eaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sg-matrix-factorization\n",
      "INFO:sagemaker:Deleting endpoint with name: sg-matrix-factorization\n"
     ]
    }
   ],
   "source": [
    "# endpoint delete\n",
    "# fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634936da-6b5d-4c31-8fd9-205f004733e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
